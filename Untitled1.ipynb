{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79926788",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72cdced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_end, time_start = None, None\n",
    "\n",
    "def clock_reset():\n",
    "    global time_end, time_start\n",
    "    time_end, time_start = None, time.perf_counter()\n",
    "\n",
    "def clock(label):\n",
    "    global time_end, time_start\n",
    "    assert time_start is not None\n",
    "    time_end = time.perf_counter()\n",
    "    print(\"\\r{}: {:.2f} s\".format(label, time_end - time_start))\n",
    "    time_start = time_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_df(df):\n",
    "    print(f\"Shape: {df.shape}, Memory: {df.memory_usage().sum() // 1024 // 1024} MB, Columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7fce18",
   "metadata": {},
   "source": [
    "# MySQL to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert MySQL trace to PostgreSQL trace.\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "PG_COLS = [\n",
    "    \"log_time\",\n",
    "    \"user_name\",\n",
    "    \"database_name\",\n",
    "    \"process_id\",\n",
    "    \"connection_from\",\n",
    "    \"session_id\",\n",
    "    \"session_line_num\",\n",
    "    \"command_tag\",\n",
    "    \"session_start_time\",\n",
    "    \"virtual_transaction_id\",\n",
    "    \"transaction_id\",\n",
    "    \"error_severity\",\n",
    "    \"sql_state_code\",\n",
    "    \"message\",\n",
    "    \"detail\",\n",
    "    \"hint\",\n",
    "    \"internal_query\",\n",
    "    \"internal_query_pos\",\n",
    "    \"context\",\n",
    "    \"query\",\n",
    "    \"query_pos\",\n",
    "    \"location\",\n",
    "    \"application_name\",\n",
    "    \"backend_type\",\n",
    "]\n",
    "\n",
    "def convert_mysql_log_to_mysql_df(log_file):\n",
    "    # Regexes for recognizing various MySQL query log constructs.\n",
    "    header_regex = re.compile(r'[\\s\\S]*Time\\s+Id\\s+Command\\s+Argument')\n",
    "    date_id_regex = re.compile(r'^(\\d+.*)Z(\\d+)')\n",
    "    full_regex = re.compile(r'(\\d+.*)Z(\\d+) (Connect|Init DB|Query|Quit|Statistics)\\t([\\s\\S]*)')\n",
    "    \n",
    "    # Extract the rows from the query log.\n",
    "    rows = []\n",
    "    seen = set()\n",
    "    with open(log_file, 'r', encoding='latin-1') as f:\n",
    "        buffer = []\n",
    "        # Iterate over each line in the query log as delimited by \\n.\n",
    "        for line in f:\n",
    "            # First, remove any trailing \\n's.\n",
    "            line = line.rstrip('\\n')\n",
    "            \n",
    "            # If there is no date, this is _probably_ part of the previous line.\n",
    "            if date_id_regex.match(line) is None:\n",
    "                # Continuation of previous line.\n",
    "                buffer.append(line)\n",
    "                continue\n",
    "\n",
    "            # Otherwise, finish the current line and initialize the next.\n",
    "            joined_buf = ' '.join(buffer)\n",
    "            buffer = [line]\n",
    "\n",
    "            # PostgreSQL vs MySQL things.\n",
    "            joined_buf = joined_buf.replace(\"\\'\", \"'\")\n",
    "            \n",
    "            # Parse the current line into a row.\n",
    "            match = full_regex.match(joined_buf)\n",
    "            if match is None:\n",
    "                assert header_regex.match(joined_buf) is not None, f\"Bad line: {joined_buf}\"\n",
    "            else:\n",
    "                rows.append(match.groups())\n",
    "\n",
    "    # Construct a dataframe out of the rows.\n",
    "    raw_df = pd.DataFrame(rows, columns=['Time', 'Id', 'Command', 'Argument'])\n",
    "    command_set = set(raw_df['Command'])\n",
    "    known_command_set = set('Connect|Init DB|Query|Quit|Statistics'.split('|'))\n",
    "    assert command_set.issubset(known_command_set), f\"Bad command set: {command_set}\"\n",
    "    return raw_df\n",
    "\n",
    "def convert_mysql_df_to_postgresql_df(mysql_df):\n",
    "    dfs = []\n",
    "    gb = mysql_df.sort_values(['Time', 'Id']).groupby('Id')\n",
    "    for group, group_vals in gb:\n",
    "        thread_id, df = group, group_vals\n",
    "        # TODO(WAN): Right now, we assume autocommit=1. But maybe we can parse this out.\n",
    "        df['session_id'] = thread_id\n",
    "        df['session_line_num'] = range(df.shape[0])\n",
    "        df['virtual_transaction_id'] = [f'AAC/{thread_id}/{n}' for n in range(df.shape[0])]\n",
    "        df.drop(columns=['Id'], inplace=True)\n",
    "        # TODO(WAN): This is kind of an abuse of PostgreSQL portal names.\n",
    "        df['message'] = 'execute ' + df['Command'] + ': ' + df['Argument']\n",
    "        df.drop(columns=['Command', 'Argument'], inplace=True)\n",
    "        df.rename(columns={'Time': 'log_time'}, inplace=True)\n",
    "        dfs.append(df)\n",
    "\n",
    "    big_df = pd.concat(dfs).sort_values(['log_time'])\n",
    "    reindexed = big_df.reindex(columns=PG_COLS, fill_value='')\n",
    "    return reindexed\n",
    "\n",
    "log_file = Path('/home/kapi/admissions/magneto.log.2016-09-04')\n",
    "csv_file = Path(f'postgresql_{log_file.name}.csv')\n",
    "\n",
    "clock_reset()\n",
    "mysql_df = convert_mysql_log_to_mysql_df(log_file)\n",
    "clock(\"Convert MySQL log to MySQL df\")\n",
    "describe_df(mysql_df)\n",
    "\n",
    "clock_reset()\n",
    "postgresql_df = convert_mysql_df_to_postgresql_df(mysql_df)\n",
    "clock(\"Convert MySQL df to PostgreSQL df\")\n",
    "describe_df(postgresql_df)\n",
    "del mysql_df\n",
    "\n",
    "clock_reset()\n",
    "postgresql_df.to_csv(csv_file, index=False, header=False, quoting=csv.QUOTE_ALL)\n",
    "clock(\"Write PostgreSQL df to CSV\")\n",
    "del postgresql_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8173b2",
   "metadata": {},
   "source": [
    "# Invoke preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6101faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(csv_file, 'r') as f:\n",
    "    ctr = 1\n",
    "    lines = []\n",
    "    for i, line in enumerate(f, 1):\n",
    "        lines.append(line)\n",
    "        if i % 100000 == 0:\n",
    "            with open(f\"{csv_file.stem}_{ctr}.csv\", \"w\") as outfile:\n",
    "                outfile.writelines(lines)\n",
    "                lines = []\n",
    "            ctr += 1\n",
    "    if len(lines) > 0:\n",
    "        with open(f\"{csv_file.stem}_{ctr}.csv\", \"w\") as outfile:\n",
    "                outfile.writelines(lines)\n",
    "                lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2880708",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script env csv_file=\"postgresql_magneto.log.2016-09-04_1.csv\" bash\n",
    "\n",
    "set -eux\n",
    "rm -rf ./tmp/\n",
    "mkdir -p ./tmp/\n",
    "cp \"$csv_file\" ./tmp/\n",
    "python3 ./forecast/preprocessor.py --query-log-folder ./tmp --output-parquet out.parquet.gzip --output-timestamp out.timestamp.txt --output-query-templates out.templates.txt > preprocessor.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35839c10",
   "metadata": {},
   "source": [
    "# Load preprocessor back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1adfb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecast.preprocessor import Preprocessor\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414dcde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed(parquet):\n",
    "    preprocessor = Preprocessor(parquet_path=parquet)\n",
    "    df = preprocessor.get_dataframe()\n",
    "    empties = df['query_template'] == ''\n",
    "    print(f\"Removing {sum(empties)} empty query template values.\")\n",
    "    df = df[:][~empties]\n",
    "    df.shape\n",
    "    return df\n",
    "\n",
    "parquet = r'./out.parquet.gzip' # admissions\n",
    "# parquet = r'./preprocessed.parquet.gzip' # tpcc\n",
    "df = load_preprocessed(parquet)\n",
    "describe_df(df)\n",
    "display(df)\n",
    "print(f\"{len(set(df['query_template']))} unique query templates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfdf5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pomegranate import DiscreteDistribution, ConditionalProbabilityTable, MarkovChain\n",
    "\n",
    "from multiprocessing import Pool, Manager, cpu_count\n",
    "\n",
    "mc_vtxid = False\n",
    "\n",
    "def build_label_encoder(df):\n",
    "    clock_reset()\n",
    "    le = LabelEncoder()\n",
    "    target = df['query_template'].values\n",
    "    if not mc_vtxid:\n",
    "        target = np.concatenate([['SESSION_BEGIN', 'SESSION_END'], target])\n",
    "    le.fit(target)\n",
    "    df['query_template_enc'] = le.transform(df['query_template'])\n",
    "    clock('LabelEncoder')\n",
    "    return le\n",
    "\n",
    "def grouper(item):\n",
    "    group_id, group = item\n",
    "    group = group.sort_values(['log_time', 'session_line_num'])\n",
    "    group_vals = group['query_template_enc'].values\n",
    "    if not mc_vtxid:\n",
    "        group_vals = np.concatenate([[le.transform(['SESSION_BEGIN'])[0]], group_vals, [le.transform(['SESSION_END'])[0]]])\n",
    "    return group_id, group_vals\n",
    "\n",
    "def build_markov_chain(df, le):\n",
    "    clock_reset()\n",
    "    \n",
    "    if mc_vtxid:\n",
    "        groups = df.groupby('virtual_transaction_id')\n",
    "    else:\n",
    "        groups = df.groupby('session_id')\n",
    "    clock(f'Grouping ({len(groups)} groups)')\n",
    "    \n",
    "    ret_list = None\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        ret_list = pool.map(grouper, groups)\n",
    "    data = {k: v for k, v in ret_list}\n",
    "    print(len(data), \" unique transactions\")\n",
    "    clock('Grouping and building up data')\n",
    "\n",
    "    trajs = set()\n",
    "    for values in data.values():\n",
    "        trajs.add(','.join(str(v) for v in values.tolist()))\n",
    "    print(len(trajs), \" unique trajectories\")\n",
    "    clock('Computing trajectories (optional)')\n",
    "\n",
    "    n = len(le.classes_)\n",
    "    startdist = {i : 0 for i in range(n)}\n",
    "    if mc_vtxid:\n",
    "        if 'BEGIN' in le.classes_:\n",
    "            startdist[le.transform(['BEGIN'])[0]] = 1.0\n",
    "    dist = DiscreteDistribution(startdist)\n",
    "\n",
    "    cptt = [[i, j, 1 if i == j else 0] for i in range(n) for j in range(n)]\n",
    "    cpt = ConditionalProbabilityTable(cptt, [dist])\n",
    "\n",
    "    mc = MarkovChain([dist, cpt])\n",
    "    clock(\"Building Markov Chain\")\n",
    "    \n",
    "    samples = list(data.values())\n",
    "    mc = mc.from_samples(samples)\n",
    "\n",
    "    cpt = mc.distributions[1]\n",
    "    cptd = cpt.to_dict()\n",
    "    dada = pd.DataFrame(cptd['table'], columns=['src','dst','val'])\n",
    "    accs = []\n",
    "    for i in range(n):\n",
    "        dasub = dada[dada['src'] == str(i)]\n",
    "        if dasub['val'].nunique() == 1:\n",
    "            accs.append(str(i))\n",
    "\n",
    "    dada.loc[dada['src'].isin(accs), 'val'] = 0\n",
    "    for i in accs:\n",
    "        dada.loc[(dada['src'] == str(i)) & (dada['dst'] == str(i)), 'val'] = 1\n",
    "    dadal = dada.to_dict(orient='list')\n",
    "\n",
    "    sdv = [[s,d,v] for s,d,v in zip(dadal['src'], dadal['dst'], dadal['val'])]\n",
    "    cptd['table'] = sdv\n",
    "    mc.distributions[1] = cpt.from_dict(cptd)\n",
    "\n",
    "    print('Had to fix distribution for ')\n",
    "    print(le.inverse_transform([int(x) for x in accs]))\n",
    "    clock(\"Fixing Markov Chain\")\n",
    "    \n",
    "    return mc\n",
    "\n",
    "le = build_label_encoder(df)\n",
    "mc = build_markov_chain(df, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85979914",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from graphviz import Digraph, escape\n",
    "\n",
    "\n",
    "def printer(string, every=24):\n",
    "    return '\\n'.join(string[i:i+every] for i in range(0, len(string), every))\n",
    "\n",
    "def draw_graph(table, label_encoder):\n",
    "    f = Digraph('graph', filename='graph', format='pdf')\n",
    "    \n",
    "    letters = np.unique(table[:, 0])\n",
    "\n",
    "    def inv(s):\n",
    "        if True:\n",
    "            return s\n",
    "        return printer(le.inverse_transform([int(s)])[0])\n",
    "    \n",
    "    for state in letters:\n",
    "        f.node(inv(state))\n",
    "\n",
    "    for row in table:\n",
    "        if float(row[2]) > 0.00001:\n",
    "            src = inv(row[0])\n",
    "            dst = inv(row[1])\n",
    "            \n",
    "            if mc_vtxid:\n",
    "                if 'COMMIT' in le.classes_:\n",
    "                    if int(row[0]) in le.transform(['COMMIT']):\n",
    "                        assert row[0] == row[1] or float(row[2]) <= 0.001, row\n",
    "                        continue\n",
    "                if 'ROLLBACK' in le.classes_:\n",
    "                    if int(row[0]) in le.transform(['ROLLBACK']):\n",
    "                        assert row[0] == row[1] or float(row[2]) <= 0.001, row\n",
    "                        continue\n",
    "                if 'BEGIN' in le.classes_:\n",
    "                    if int(row[1]) in le.transform(['BEGIN']):\n",
    "                        assert row[0] == row[1] or float(row[2]) <= 0.001, row\n",
    "                        continue\n",
    "            f.edge(src, dst, label=f'{float(row[2]):.2f}', penwidth=f'{2.5*float(row[2])}')\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "import json\n",
    "table = np.array(json.loads(mc.distributions[1].to_json())['table'])\n",
    "g = draw_graph(table, le)\n",
    "g.render(directory='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb444c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "le.inverse_transform([139, 6, 14, 47, 55, 167, 116, 48, 172, 156, 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffad4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
